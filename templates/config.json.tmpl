{
  # Data Lake (S3) region
  # This field is optional if it can be resolved with AWS region provider chain.
  # It checks places like env variables, system properties, AWS profile file.
  # https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/regions/providers/DefaultAwsRegionProviderChain.html
  "region": "${region}",

  # SQS topic name used by Transformer and Loader to communicate
  "messageQueue": "${message_queue}",

  # Warehouse connection details
  "storage" : {
    "type": "databricks",
    # Hostname of Databricks cluster
    "host": "${db_host}",
    # Optional. Override the Databricks default catalog, e.g. with a Unity catalog name.
    "catalog": "${db_catalog}",
    # DB schema
    "schema": "${db_schema}",
    # Database port
    "port": ${db_port},
    # Http Path of Databricks cluster
    "httpPath": "${db_http_path}",
    # Databricks Authentication Token
    "password": "${db_auth_token}"
    # User agent name for Databricks connection. Optional, default value "snowplow-rdbloader-oss"
    "userAgent": "snowplow-rdbloader-oss"

    # Optimize period per table, that will be used as predicate for the OPTIMIZE command.
    "eventsOptimizePeriod": "2 days",

    # Optional, default method is 'NoCreds'
    # Specifies the auth method to use with 'COPY INTO' statement.
    "loadAuthMethod": {
      # With 'TempCreds', temporary credentials will be created for every
      # load operation and these temporary credentials will be passed to
      # 'COPY INTO' statement. With this way, Databricks cluster doesn't need
      # permission to access to transformer output S3 bucket.
      # This access will be provided by temporary credentials.
      "type": "TempCreds"
      # IAM role that is used while creating temporary credentials
      # Created credentials will allow to access resources specified in the given role
      # In our case, “s3:GetObject*”, “s3:ListBucket”, and “s3:GetBucketLocation” permissions
      # for transformer output S3 bucket should be specified in the role.
      "roleArn": "${temp_credentials_role_arn}"
    }
  },

  "schedules": {
    # Periodic schedules to stop loading, e.g. for Redshift maintenance window
    # Any amount of schedules is supported, but recommended to not overlap them
    # The schedule works with machine's local timezone (and UTC is recommended)
    "noOperation": [
      {
        # Human-readable name of the no-op window
        "name": "Maintenance window",
        # Cron expression with second granularity
        "when": "0 0 12 * * ?",
        # For how long the loader should be paused
        "duration": "1 hour"
      }
    ],
    # Loader runs periodic OPTIMIZE statements to prevent growing number of files behind delta tables.
    "optimizeEvents": "0 0 0 ? * *",
    "optimizeManifest": "0 0 5 ? * *"
  }

  # Observability and reporting options
  "monitoring": {
%{ if sp_tracking_enabled ~}
    # Snowplow tracking (optional)
    "snowplow": {
      "appId": "${sp_tracking_app_id}",
      "collector": "${sp_tracking_collector_url}"
    },
%{ endif ~}

%{ if webhook_enabled ~}
    # An endpoint for alerts and informational events
    # Everything sent to snowplow collector (as properly formed self-describing events)
    # will also be sent to the webhook as POST payloads with self-describing JSONs
    "webhook": {
      # An actual HTTP endpoint
      "endpoint": "${webhook_collector}"
    },
%{ endif ~}

%{ if sentry_enabled ~}
    # Optional, for tracking runtime exceptions
    "sentry": {
      "dsn": "${sentry_dsn}"
    },
%{ endif ~}

    # Optional, configure how metrics are reported
    "metrics": {
%{ if statsd_enabled ~}
      # Optional, send metrics to StatsD server
      "statsd": {
        "hostname": "${statsd_host}",
        "port": ${statsd_port},
      },
%{ endif ~}

      # Optional, print metrics on stdout (with slf4j)
%{ if stdout_metrics_enabled ~}
      "stdout": {
        # Optional, override the default metric prefix
        # "prefix": "snowplow.rdbloader."
      }
%{ endif ~}

      # Optional, period for metrics emitted periodically
      # Default value 5 minutes
      # There is only one periodic metric at the moment.
      # This metric is minimum_age_of_loaded_data.
      # It specifies how old is the latest event in the warehouse.
      "period": "5 minutes"
    },

%{ if folder_monitoring_enabled ~}
    # Optional, configuration for periodic unloaded/corrupted folders checks
    "folders": {
      # Path where Loader could store auxiliary logs
      # Loader should be able to write here, Redshift should be able to load from here
      "staging": "${folder_monitoring_staging}",
      # How often to check
      "period": "${folder_monitoring_period}",
      "since": "${folder_monitoring_since}",
      "until": "${folder_monitoring_until}",
      # Path to transformer archive (must be same as Transformer's `output.path`)
      "transformerOutput": "${transformer_output}"
    }
%{ endif ~}

%{ if health_check_enabled ~}
    # Periodic DB health-check, raising a warning if DB hasn't responded to `SELECT 1`
    "healthCheck": {
      # How often query a DB
      "frequency": "${health_check_freq}",
      # How long to wait for a response
      "timeout": "${health_check_timeout}"
    }
%{ endif ~}
  },

%{ if retry_queue_enabled ~}
  # Additional backlog of recently failed folders that could be automatically retried
  # Retry Queue saves a failed folder and then re-reads the info from shredding_complete S3 file
  "retryQueue": {
    # How often batch of failed folders should be pulled into a discovery queue
    "period": "${retry_period}",
    # How many failures should be kept in memory
    # After the limit is reached new failures are dropped
    "size": ${retry_queue_size},
    # How many attempt to make for each folder
    # After the limit is reached new failures are dropped
    "maxAttempts": ${retry_queue_max_attempt},
    # Artificial pause after each failed folder being added to the queue
    "interval": "${retry_queue_interval}"
  }
%{ endif ~}

  "telemetry": {
    "disable": ${telemetry_disable}
    "interval": 15 minutes
    "method": "POST"
    "collectorUri": "${telemetry_collector_uri}"
    "collectorPort": ${telemetry_collector_port}
    "secure": ${telemetry_collector_port}
    "userProvidedId": "${telemetry_user_provided_id}"
    "autoGeneratedId": "${telemetry_auto_gen_id}"
    "instanceId": "$${TELEMETRY_INSTANCE_ID}"
    "moduleName": "${telemetry_module_name}"
    "moduleVersion": "${telemetry_module_version}"
  }
}
